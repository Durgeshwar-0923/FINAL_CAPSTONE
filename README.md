workspace strucuture
```text.
.
‚îú‚îÄ‚îÄ Dockerfile                 # Instructions to build the container image for deployment
‚îú‚îÄ‚îÄ README.md                  # Project overview and instructions
‚îú‚îÄ‚îÄ Untitled.ipynb             # Scratch or testing notebook
‚îú‚îÄ‚îÄ airflow/                   # Airflow orchestration for automation
‚îÇ   ‚îú‚îÄ‚îÄ dags/                  # DAGs: data pipeline & retraining workflows
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline.py   # DAG for ETL, loading new data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retrain_pipeline.py # DAG for triggering model retraining on drift
‚îÇ   ‚îî‚îÄ‚îÄ plugins/               # Custom Airflow plugins (empty placeholder or custom ops)
‚îú‚îÄ‚îÄ catboost_info/             # CatBoost training logs and metadata
‚îÇ   ‚îú‚îÄ‚îÄ catboost_training.json # CatBoost training info
‚îÇ   ‚îú‚îÄ‚îÄ learn/                 # CatBoost learn events
‚îÇ   ‚îú‚îÄ‚îÄ learn_error.tsv        # CatBoost training error metrics
‚îÇ   ‚îî‚îÄ‚îÄ time_left.tsv          # Estimated time left for training
‚îú‚îÄ‚îÄ data/                      # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ drift_reports/         # Drift report outputs (e.g., Evidently)
‚îÇ   ‚îú‚îÄ‚îÄ processed/             # Processed datasets ready for modeling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 13_final_features.csv # Final features dataset
‚îÇ   ‚îî‚îÄ‚îÄ raw/                   # Raw input data files
‚îÇ       ‚îú‚îÄ‚îÄ Lead Scoring.csv   # Original lead scoring data
‚îÇ       ‚îî‚îÄ‚îÄ staged_lead_scoring_data.csv # Data staged for ETL
‚îú‚îÄ‚îÄ deploy.py                  # Script for deploying model container to ECR/SageMaker
‚îú‚îÄ‚îÄ docker/                    # Docker deployment files
‚îÇ   ‚îú‚îÄ‚îÄ app.py                 # Flask app for serving predictions
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml     # Docker Compose config for local multi-container runs
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf             # NGINX config for reverse proxying Flask app
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py        # Preprocessing logic for incoming requests
‚îÇ   ‚îú‚îÄ‚îÄ serve/                 # (Likely) Static/WSGI files for serving app
‚îÇ   ‚îú‚îÄ‚îÄ untitled.py            # (Likely) Placeholder or scratch file
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py                # WSGI entry point for Flask in production
‚îú‚îÄ‚îÄ evidently_reports/         # Generated drift and data quality reports by Evidently
‚îÇ   ‚îú‚îÄ‚îÄ *.json, *.html         # Different timestamps: train vs test, prediction vs training
‚îú‚îÄ‚îÄ flask.txt                  # Requirements for Flask app (or notes)
‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks for EDA and experimentation
‚îÇ   ‚îú‚îÄ‚îÄ exploratory_analysis.ipynb # Initial EDA notebook
‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.py    # Script version of EDA
‚îú‚îÄ‚îÄ outputs/                   # Outputs: plots, feature importances, Optuna trials, shap plots
‚îÇ   ‚îú‚îÄ‚îÄ *.png                  # Feature importance, SHAP summary plots
‚îÇ   ‚îú‚îÄ‚îÄ *.csv                  # Optuna tuning results, model summaries
‚îÇ   ‚îú‚îÄ‚îÄ *.html                 # Optuna history visualizations
‚îÇ   ‚îú‚îÄ‚îÄ stages/                # Intermediate preprocessing outputs for pipeline stages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *.csv              # Staged feature engineering steps
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *.json             # Columns removed/selected at each stage
‚îú‚îÄ‚îÄ reports/                   # EDA or final project reports
‚îÇ   ‚îî‚îÄ‚îÄ eda_report.html        # Full EDA report in HTML
‚îú‚îÄ‚îÄ requirements.txt           # Project dependencies
‚îú‚îÄ‚îÄ requirementstwo.txt        # Alternate requirements file (or dev env)
‚îú‚îÄ‚îÄ run.py                     # Entry script to run the project locally
‚îú‚îÄ‚îÄ sagemaker/                 # SageMaker experiment tracking outputs (MLflow)
‚îÇ   ‚îú‚îÄ‚îÄ */                     # Run folders with meta.yaml and metrics
‚îÇ   ‚îî‚îÄ‚îÄ meta.yaml              # Experiment metadata
‚îú‚îÄ‚îÄ setup.py                   # Python package setup if needed for pip install
‚îú‚îÄ‚îÄ src/                       # Core source code
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # Flask API logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Main API entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prediction_service.py # Handles prediction calls
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/         # HTML templates for UI
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ index.html     # Input form page
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ result.html    # Result display page
‚îÇ   ‚îú‚îÄ‚îÄ config/                # Configuration files
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py          # App and pipeline configs
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion/        # Data ingestion layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py     # Load data from S3/Redshift
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database_operations.py # DB ops for Redshift
‚îÇ   ‚îú‚îÄ‚îÄ data_processing/       # Data cleaning and feature engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eda.py             # EDA utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_selector.py # Feature selection logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor/      # Detailed preprocessing modules
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ binning.py, cleaning.py, clustering.py, etc.
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Model training & tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optuna_tuner.py    # Hyperparameter tuning with Optuna
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_models.py    # Training logic
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/            # Model & data drift monitoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ drift_detector.py  # Drift detection logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_monitor.py   # General monitoring tasks
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Helper utilities
‚îÇ       ‚îú‚îÄ‚îÄ logger.py          # Logging utility
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py         # Custom metrics calculation
‚îÇ       ‚îî‚îÄ‚îÄ mlflow_logger.py   # MLflow logging utilities
‚îú‚îÄ‚îÄ structure.txt              # Text description of project structure (this!)
‚îú‚îÄ‚îÄ test.py                    # Example or scratch test script
‚îú‚îÄ‚îÄ tests/                     # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py            # API tests
‚îÇ   ‚îú‚îÄ‚îÄ test_data_processing.py # Preprocessing tests
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py         # Model unit tests
‚îú‚îÄ‚îÄ untitled1.txt              # Notes or scratch
‚îî‚îÄ‚îÄ your_mlflow_tracking_uri_here # Placeholder for MLflow tracking URI

  ```
This repository contains a scalable, production-ready machine learning pipeline designed to automate lead scoring and help sales teams prioritize high-value leads more effectively. Built entirely on AWS cloud services, the system ensures robust ETL, reliable training, automated deployment, and continuous monitoring with MLOps best practices.

---

## üßê Business Problem
Sales teams often spend significant time and resources following up with low-quality leads, leading to lost opportunities and lower conversion rates. This project solves that by building an accurate lead scoring model that continuously adapts to data changes, helping the business focus efforts where they matter most.

---

## üéØ Objectives
- Automate lead scoring with a reliable ML model.
- Maintain high prediction accuracy (AUC > 0.97).
- Ensure secure, scalable data storage and processing.
- Automate deployment, monitoring, and retraining to handle drift.
- Deliver predictions through an easy-to-use web interface.

---

## üóÇÔ∏è Architecture Overview

**Key stages of the pipeline:**

1. **Data Storage:**  
   - Raw and new prediction data is stored in **Amazon S3**.

2. **ETL & Data Cataloging:**  
   - **AWS Glue Crawlers** scan the S3 bucket to create a Data Catalog.
   - **AWS Glue ETL Jobs** transform and load data into **Amazon Redshift**.

3. **Model Development:**  
   - **Amazon SageMaker** connects to Redshift, loads data, performs preprocessing, and trains multiple models.
   - **MLflow Tracking Server** logs experiments, metrics, and artifacts.

4. **Model Versioning:**  
   - The best-performing model is registered in the **MLflow Model Registry** with a `Champion` alias.

5. **Containerization & Deployment:**  
   - The model and artifacts are packaged using **Docker** and **NGINX**, pushed to **Amazon ECR**.
   - **SageMaker Endpoints** deploy the latest container for real-time inference.

6. **Serving:**  
   - A **Flask UI** connects to the SageMaker Endpoint, allowing users to submit leads and get predictions.

7. **Monitoring & Retraining:**  
   - **Apache Airflow** schedules drift detection and automatically triggers retraining and redeployment when needed.

---

## ‚öôÔ∏è Tech Stack

- **AWS S3** ‚Äî Raw data storage  
- **AWS Glue** ‚Äî ETL and data cataloging  
- **Amazon Redshift** ‚Äî Data warehouse  
- **Amazon SageMaker** ‚Äî Model development & hosting  
- **MLflow** ‚Äî Experiment tracking & model registry  
- **Docker & NGINX** ‚Äî Containerization  
- **Amazon ECR** ‚Äî Container registry  
- **Apache Airflow** ‚Äî Orchestration & monitoring  
- **Flask** ‚Äî User-facing prediction UI  
- **Python** ‚Äî Core language for data processing & modeling
